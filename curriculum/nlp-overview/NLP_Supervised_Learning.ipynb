{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP for Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in cappuccino cup review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('coffee.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 542 reviews in the dataset\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While most reviewers leave a 5 star review, there are also a good portion leaving 1 star reviews\n",
    "data.stars.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Remove 3 star reviews\n",
    "data = data[data.stars!=3]\n",
    "\n",
    "# Set 4/5 star reviews to positive, the rest to negative\n",
    "data['sentiment'] = np.where(data['stars'] >= 4, 'positive', 'negative')\n",
    "\n",
    "# Include only the sentiment and reviews columns\n",
    "data = data[['sentiment', 'reviews']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of the new dataset\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the dataset has mostly positive reviews\n",
    "data.sentiment.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing steps - remove numbers, captial letters and punctuation\n",
    "import re\n",
    "import string\n",
    "\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "\n",
    "data['reviews'] = data.reviews.map(alphanumeric).map(punc_lower)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y data sets\n",
    "X = data.reviews\n",
    "y = data.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first document-term matrix has default Count Vectorizer values - counts of unigrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv1 = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_train_cv1 = cv1.fit_transform(X_train)\n",
    "X_test_cv1  = cv1.transform(X_test)\n",
    "\n",
    "pd.DataFrame(X_train_cv1.toarray(), columns=cv1.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second document-term matrix has both unigrams and bigrams, and indicators instead of counts\n",
    "cv2 = CountVectorizer(ngram_range=(1,2), binary=True, stop_words='english')\n",
    "\n",
    "X_train_cv2 = cv2.fit_transform(X_train)\n",
    "X_test_cv2  = cv2.transform(X_test)\n",
    "\n",
    "pd.DataFrame(X_train_cv2.toarray(), columns=cv2.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try classifying using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logistic regression model to use\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the first model\n",
    "lr.fit(X_train_cv1, y_train)\n",
    "y_pred_cv1 = lr.predict(X_test_cv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the second model\n",
    "lr.fit(X_train_cv2, y_train)\n",
    "y_pred_cv2 = lr.predict(X_test_cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate the error metrics, since we'll be doing this several times\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def conf_matrix(actual, predicted):\n",
    "    cm = confusion_matrix(actual, predicted)\n",
    "    sns.heatmap(cm, xticklabels=['predicted_negative', 'predicted_positive'], \n",
    "                yticklabels=['actual_negative', 'actual_positive'], annot=True,\n",
    "                fmt='d', annot_kws={'fontsize':20}, cmap=\"YlGnBu\");\n",
    "\n",
    "    true_neg, false_pos = cm[0]\n",
    "    false_neg, true_pos = cm[1]\n",
    "\n",
    "    accuracy = round((true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg),3)\n",
    "    precision = round((true_pos) / (true_pos + false_pos),3)\n",
    "    recall = round((true_pos) / (true_pos + false_neg),3)\n",
    "    f1 = round(2 * (precision * recall) / (precision + recall),3)\n",
    "\n",
    "    cm_results = [accuracy, precision, recall, f1]\n",
    "    return cm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The heat map for the first logistic regression model\n",
    "cm1 = conf_matrix(y_test, y_pred_cv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The heat map for the second logistic regression model\n",
    "cm2 = conf_matrix(y_test, y_pred_cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all of the error metrics into a dataframe for comparison\n",
    "results = pd.DataFrame(list(zip(cm1, cm2)))\n",
    "results = results.set_index([['Accuracy', 'Precision', 'Recall', 'F1 Score']])\n",
    "results.columns = ['LogReg1', 'LogReg2']\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the two models, the first model has better precision, while the second model has better accuracy and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try classifying using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the first Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_cv1, y_train)\n",
    "\n",
    "y_pred_cv1_nb = mnb.predict(X_test_cv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the second Naive Bayes model\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_cv2, y_train)\n",
    "\n",
    "y_pred_cv2_nb = bnb.predict(X_test_cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the heat map for the first Naive Bayes model\n",
    "cm3 = conf_matrix(y_test, y_pred_cv1_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the heat map for the second Naive Bayes model\n",
    "cm4 = conf_matrix(y_test, y_pred_cv2_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all of the error metrics into a dataframe for comparison\n",
    "results_nb = pd.DataFrame(list(zip(cm3, cm4)))\n",
    "results_nb = results_nb.set_index([['Accuracy', 'Precision', 'Recall', 'F1 Score']])\n",
    "results_nb.columns = ['NB1', 'NB2']\n",
    "results_nb\n",
    "\n",
    "results = pd.concat([results, results_nb], axis=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first Naive Bayes model outperforms both Logistic Regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try using TF-IDF instead of Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF versions of the Count Vectorizers created earlier in the exercise\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf1 = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf1 = tfidf1.fit_transform(X_train)\n",
    "X_test_tfidf1  = tfidf1.transform(X_test)\n",
    "\n",
    "tfidf2 = TfidfVectorizer(ngram_range=(1,2), binary=True, stop_words='english')\n",
    "X_train_tfidf2 = tfidf2.fit_transform(X_train)\n",
    "X_test_tfidf2  = tfidf2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the first logistic regression on the TF-IDF data\n",
    "lr.fit(X_train_tfidf1, y_train)\n",
    "y_pred_tfidf1_lr = lr.predict(X_test_tfidf1)\n",
    "cm5 = conf_matrix(y_test, y_pred_tfidf1_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the second logistic regression on the TF-IDF data\n",
    "lr.fit(X_train_tfidf2, y_train)\n",
    "y_pred_tfidf2_lr = lr.predict(X_test_tfidf2)\n",
    "cm6 = conf_matrix(y_test, y_pred_tfidf2_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the first Naive Bayes model on the TF-IDF data\n",
    "mnb.fit(X_train_tfidf1.toarray(), y_train)\n",
    "y_pred_tfidf1_nb = mnb.predict(X_test_tfidf1)\n",
    "cm7 = conf_matrix(y_test, y_pred_tfidf1_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the second Naive Bayes model on the TF-IDF data\n",
    "bnb.fit(X_train_tfidf2.toarray(), y_train)\n",
    "y_pred_tfidf2_nb = bnb.predict(X_test_tfidf2)\n",
    "cm8 = conf_matrix(y_test, y_pred_tfidf2_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all of the error metrics into a dataframe for comparison\n",
    "results_tf = pd.DataFrame(list(zip(cm5, cm6, cm7, cm8)))\n",
    "results_tf = results_tf.set_index([['Accuracy', 'Precision', 'Recall', 'F1 Score']])\n",
    "results_tf.columns = ['LR1-TFIDF', 'LR2-TFIDF', 'NB1-TFIDF', 'NB2-TFIDF']\n",
    "results_tf\n",
    "\n",
    "results = pd.concat([results, results_tf], axis=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like using TF-IDF, we were able to improve the recall, but the accuracy and precision of the first Naive Bayes model still outperforms the other models.\n",
    "\n",
    "Overall, the first Naive Bayes model (using unigrams and counts) seems to best classify positive and negative cappuccino cup reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis]",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
